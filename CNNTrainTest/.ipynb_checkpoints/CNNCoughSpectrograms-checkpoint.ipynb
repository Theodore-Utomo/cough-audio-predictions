{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b554e78-ff00-4769-bd8d-663b3da9155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20664/20664 [34:37<00:00,  9.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# === Settings ===\n",
    "csv_path = 'metadata_compiled_dummies.csv'\n",
    "base_image_dir = '../YuanDataProcessing'\n",
    "img_x = 600\n",
    "img_y = 240\n",
    "img_size = (img_x, img_y)\n",
    "\n",
    "# === Load CSV and preprocess ===\n",
    "df = pd.read_csv(csv_path)[['uuid', 'status_COVID-19', 'status_healthy', 'status_symptomatic']]\n",
    "df = df.dropna(subset=['status_COVID-19', 'status_healthy', 'status_symptomatic'])\n",
    "df[['status_COVID-19', 'status_healthy', 'status_symptomatic']] = df[['status_COVID-19', 'status_healthy', 'status_symptomatic']].astype(int)\n",
    "\n",
    "# === Map UUIDs to file paths ===\n",
    "all_image_paths = glob.glob(os.path.join(base_image_dir, 'folder_*', '*.png'))\n",
    "uuid_to_path = {os.path.splitext(os.path.basename(p))[0]: p for p in all_image_paths}\n",
    "\n",
    "# === Load and preprocess images ===\n",
    "X, y = [], []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    uuid = row['uuid']\n",
    "    if uuid in uuid_to_path:\n",
    "        img = load_img(uuid_to_path[uuid], target_size=img_size)\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        X.append(img_array)\n",
    "        y.append(row[['status_COVID-19', 'status_healthy', 'status_symptomatic']].values)\n",
    "    else:\n",
    "        print(f\"Missing image for UUID: {uuid}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ce29b1-b179-4711-980d-99739d51f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'X_{img_x}x{img_y}.npy', X)\n",
    "np.save(f'y_{img_x}x{img_y}.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4433e1af-ae05-4928-93fb-1809e6a984d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fe572-f073-40bf-bde4-708e759cd4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# === Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y.argmax(axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# === CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train ===\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === Evaluate ===\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94be264a-4258-4b2e-9a56-5969ddf8111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 5.237959442332066, 1: 0.4450636728320276, 2: 1.7786744136001722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.3080 - loss: 1.5195 - val_accuracy: 0.4698 - val_loss: 1.0709\n",
      "Epoch 2/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.4283 - loss: 1.0867 - val_accuracy: 0.1880 - val_loss: 1.1478\n",
      "Epoch 3/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 126ms/step - accuracy: 0.3095 - loss: 1.0913 - val_accuracy: 0.2588 - val_loss: 1.1057\n",
      "Epoch 4/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 115ms/step - accuracy: 0.3209 - loss: 1.0880 - val_accuracy: 0.3247 - val_loss: 1.1103\n",
      "Epoch 5/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 106ms/step - accuracy: 0.3490 - loss: 1.0873 - val_accuracy: 0.3936 - val_loss: 1.0812\n",
      "Epoch 6/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.3951 - loss: 1.0513 - val_accuracy: 0.3664 - val_loss: 1.0636\n",
      "Epoch 7/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.4224 - loss: 0.9847 - val_accuracy: 0.3156 - val_loss: 1.1223\n",
      "Epoch 8/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.4681 - loss: 0.8757 - val_accuracy: 0.4039 - val_loss: 1.0622\n",
      "Epoch 9/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.5403 - loss: 0.7753 - val_accuracy: 0.4347 - val_loss: 1.0128\n",
      "Epoch 10/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 107ms/step - accuracy: 0.6055 - loss: 0.6425 - val_accuracy: 0.5073 - val_loss: 0.9819\n",
      "Epoch 11/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 107ms/step - accuracy: 0.6728 - loss: 0.5347 - val_accuracy: 0.4716 - val_loss: 1.0685\n",
      "Epoch 12/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 107ms/step - accuracy: 0.7143 - loss: 0.4553 - val_accuracy: 0.5435 - val_loss: 1.0195\n",
      "Epoch 13/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.7655 - loss: 0.3821 - val_accuracy: 0.5447 - val_loss: 1.0774\n",
      "Epoch 14/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.7934 - loss: 0.3370 - val_accuracy: 0.5943 - val_loss: 1.1313\n",
      "Epoch 15/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.8258 - loss: 0.2940 - val_accuracy: 0.5683 - val_loss: 1.1531\n",
      "Epoch 16/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.8445 - loss: 0.2660 - val_accuracy: 0.5750 - val_loss: 1.2795\n",
      "Epoch 17/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.8768 - loss: 0.2160 - val_accuracy: 0.6125 - val_loss: 1.3411\n",
      "Epoch 18/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 104ms/step - accuracy: 0.8860 - loss: 0.2013 - val_accuracy: 0.5937 - val_loss: 1.3099\n",
      "Epoch 19/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 111ms/step - accuracy: 0.9027 - loss: 0.1710 - val_accuracy: 0.6052 - val_loss: 1.4552\n",
      "Epoch 20/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 114ms/step - accuracy: 0.9050 - loss: 0.1628 - val_accuracy: 0.6385 - val_loss: 1.5191\n",
      "Epoch 21/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.9103 - loss: 0.1619 - val_accuracy: 0.5925 - val_loss: 1.6095\n",
      "Epoch 22/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 105ms/step - accuracy: 0.9097 - loss: 0.1493 - val_accuracy: 0.6149 - val_loss: 1.5012\n",
      "Epoch 23/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.9271 - loss: 0.1262 - val_accuracy: 0.6040 - val_loss: 1.6114\n",
      "Epoch 24/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.9294 - loss: 0.1182 - val_accuracy: 0.6729 - val_loss: 1.8102\n",
      "Epoch 25/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.9371 - loss: 0.1195 - val_accuracy: 0.6348 - val_loss: 1.9133\n",
      "Epoch 26/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.9317 - loss: 0.1152 - val_accuracy: 0.6572 - val_loss: 1.9841\n",
      "Epoch 27/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 105ms/step - accuracy: 0.9372 - loss: 0.1082 - val_accuracy: 0.6681 - val_loss: 1.8927\n",
      "Epoch 28/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.9383 - loss: 0.1144 - val_accuracy: 0.6530 - val_loss: 1.7596\n",
      "Epoch 29/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.9429 - loss: 0.0941 - val_accuracy: 0.6560 - val_loss: 1.7730\n",
      "Epoch 30/30\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.9400 - loss: 0.1029 - val_accuracy: 0.6312 - val_loss: 1.8125\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.6250 - loss: 1.7437\n",
      "Test Accuracy: 62.57%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# === Settings ===\n",
    "csv_path = 'metadata_compiled_dummies.csv'\n",
    "base_image_dir = '../YuanDataProcessing'\n",
    "img_x = 128\n",
    "img_y = 128\n",
    "img_size = (img_x, img_y)\n",
    "\n",
    "X = np.load(f'X_{128}x{128}.npy', allow_pickle=True)\n",
    "y = np.load(f'y_{128}x{128}.npy', allow_pickle=True)\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# === Compute Class Weights ===\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# === CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train Model with Class Weights ===\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# === Evaluate Model ===\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3051ab-dc51-4b33-86ea-55cfe1645ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step\n",
      "Confusion Matrix:\n",
      "[[  14  204   45]\n",
      " [ 159 2479  457]\n",
      " [  41  641   93]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    COVID-19       0.07      0.05      0.06       263\n",
      "     Healthy       0.75      0.80      0.77      3095\n",
      " Symptomatic       0.16      0.12      0.14       775\n",
      "\n",
      "    accuracy                           0.63      4133\n",
      "   macro avg       0.32      0.32      0.32      4133\n",
      "weighted avg       0.59      0.63      0.61      4133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# === Predict on test set ===\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# === Classification Report (optional, includes precision, recall, f1) ===\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['COVID-19', 'Healthy', 'Symptomatic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20650f-e948-4e1e-b81d-10875b7cde33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 5.237959442332066, 1: 0.4450636728320276, 2: 1.7786744136001722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 105ms/step - accuracy: 0.3133 - loss: 1.1976 - val_accuracy: 0.1378 - val_loss: 1.1251\n",
      "Epoch 2/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 105ms/step - accuracy: 0.2975 - loss: 1.0701 - val_accuracy: 0.1245 - val_loss: 1.1224\n",
      "Epoch 3/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.2578 - loss: 1.1018 - val_accuracy: 0.2455 - val_loss: 1.0941\n",
      "Epoch 4/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 102ms/step - accuracy: 0.2699 - loss: 1.1083 - val_accuracy: 0.3434 - val_loss: 1.0936\n",
      "Epoch 5/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 102ms/step - accuracy: 0.3252 - loss: 1.0736 - val_accuracy: 0.2322 - val_loss: 1.1175\n",
      "Epoch 6/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.3589 - loss: 1.0546 - val_accuracy: 0.2588 - val_loss: 1.1072\n",
      "Epoch 7/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.4030 - loss: 0.9891 - val_accuracy: 0.3978 - val_loss: 1.0501\n",
      "Epoch 8/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.4524 - loss: 0.9341 - val_accuracy: 0.4021 - val_loss: 1.0413\n",
      "Epoch 9/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.5064 - loss: 0.8210 - val_accuracy: 0.4401 - val_loss: 1.0082\n",
      "Epoch 10/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 101ms/step - accuracy: 0.5735 - loss: 0.6989 - val_accuracy: 0.4637 - val_loss: 0.9970\n",
      "Epoch 11/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 103ms/step - accuracy: 0.6394 - loss: 0.6032 - val_accuracy: 0.5526 - val_loss: 0.9517\n",
      "Epoch 12/20\n",
      "\u001b[1m245/465\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 101ms/step - accuracy: 0.7030 - loss: 0.4986"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# === Settings ===\n",
    "csv_path = 'metadata_compiled_dummies.csv'\n",
    "base_image_dir = '../YuanDataProcessing'\n",
    "img_x = 128\n",
    "img_y = 128\n",
    "img_size = (img_x, img_y)\n",
    "\n",
    "X = np.load(f'X_{128}x{128}.npy', allow_pickle=True)\n",
    "y = np.load(f'y_{128}x{128}.npy', allow_pickle=True)\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# === Compute Class Weights ===\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# === CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Early Stopping Callback ===\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=5,  # Wait for 5 epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore the weights from the best epoch\n",
    ")\n",
    "\n",
    "# === Train Model with Class Weights and Early Stopping ===\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# === Evaluate Model ===\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a72638-f2df-4a23-b352-13e900adf754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
