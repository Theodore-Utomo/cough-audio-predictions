{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bfb91c-56bc-458b-9acf-eb696af75827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20664/20664 [34:15<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 5.237959442332066, 1: 0.4450636728320276, 2: 1.7786744136001722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 721ms/step - accuracy: 0.3162 - loss: 2.1124 - val_accuracy: 0.2358 - val_loss: 1.0914\n",
      "Epoch 2/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 717ms/step - accuracy: 0.2887 - loss: 1.0837 - val_accuracy: 0.1729 - val_loss: 1.1029\n",
      "Epoch 3/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 718ms/step - accuracy: 0.2671 - loss: 1.0745 - val_accuracy: 0.2872 - val_loss: 1.1016\n",
      "Epoch 4/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 737ms/step - accuracy: 0.2996 - loss: 1.0877 - val_accuracy: 0.4389 - val_loss: 1.0550\n",
      "Epoch 5/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 710ms/step - accuracy: 0.3670 - loss: 1.0456 - val_accuracy: 0.2878 - val_loss: 1.1169\n",
      "Epoch 6/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 721ms/step - accuracy: 0.4114 - loss: 0.9722 - val_accuracy: 0.3126 - val_loss: 1.0845\n",
      "Epoch 7/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 720ms/step - accuracy: 0.5054 - loss: 0.8411 - val_accuracy: 0.4075 - val_loss: 1.0730\n",
      "Epoch 8/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 716ms/step - accuracy: 0.5893 - loss: 0.6987 - val_accuracy: 0.3815 - val_loss: 1.1242\n",
      "Epoch 9/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 718ms/step - accuracy: 0.7160 - loss: 0.5024 - val_accuracy: 0.4770 - val_loss: 1.1114\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 186ms/step - accuracy: 0.4303 - loss: 1.0587\n",
      "Test Accuracy: 44.23%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# === Settings ===\n",
    "csv_path = 'metadata_compiled_dummies.csv'\n",
    "base_image_dir = '../YuanDataProcessing'\n",
    "img_x = 3000 // 6\n",
    "img_y = 1200 // 6\n",
    "img_size = (img_x, img_y)\n",
    "\n",
    "# === Load CSV and preprocess ===\n",
    "df = pd.read_csv(csv_path)[['uuid', 'status_COVID-19', 'status_healthy', 'status_symptomatic']]\n",
    "df = df.dropna(subset=['status_COVID-19', 'status_healthy', 'status_symptomatic'])\n",
    "df[['status_COVID-19', 'status_healthy', 'status_symptomatic']] = df[['status_COVID-19', 'status_healthy', 'status_symptomatic']].astype(int)\n",
    "\n",
    "# === Map UUIDs to file paths ===\n",
    "all_image_paths = glob.glob(os.path.join(base_image_dir, 'folder_*', '*.png'))\n",
    "uuid_to_path = {os.path.splitext(os.path.basename(p))[0]: p for p in all_image_paths}\n",
    "\n",
    "# === Load and preprocess images ===\n",
    "X, y = [], []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    uuid = row['uuid']\n",
    "    if uuid in uuid_to_path:\n",
    "        img = load_img(uuid_to_path[uuid], target_size=img_size)\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        X.append(img_array)\n",
    "        y.append(row[['status_COVID-19', 'status_healthy', 'status_symptomatic']].values)\n",
    "    else:\n",
    "        print(f\"Missing image for UUID: {uuid}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# X = np.load(f'X_{img_x}x{img_y}.npy', allow_pickle=True)\n",
    "# y = np.load(f'y_{img_x}x{img_y}.npy', allow_pickle=True)\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# === Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y.argmax(axis=1), random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66fb7033-5e65-4010-a19b-36dc58d0e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 5.237959442332066, 1: 0.4450636728320276, 2: 1.7786744136001722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 884ms/step - accuracy: 0.2871 - loss: 1.9672 - val_accuracy: 0.3259 - val_loss: 1.0972\n",
      "Epoch 2/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 804ms/step - accuracy: 0.2234 - loss: 1.1024 - val_accuracy: 0.4021 - val_loss: 1.0789\n",
      "Epoch 3/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 820ms/step - accuracy: 0.3137 - loss: 1.0714 - val_accuracy: 0.3458 - val_loss: 1.0806\n",
      "Epoch 4/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 808ms/step - accuracy: 0.3468 - loss: 1.0352 - val_accuracy: 0.3126 - val_loss: 1.0731\n",
      "Epoch 5/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 842ms/step - accuracy: 0.4183 - loss: 0.9724 - val_accuracy: 0.2461 - val_loss: 1.1372\n",
      "Epoch 6/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 775ms/step - accuracy: 0.4569 - loss: 0.8907 - val_accuracy: 0.4299 - val_loss: 1.0431\n",
      "Epoch 7/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 733ms/step - accuracy: 0.5915 - loss: 0.7068 - val_accuracy: 0.4480 - val_loss: 1.0663\n",
      "Epoch 8/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 734ms/step - accuracy: 0.6766 - loss: 0.5441 - val_accuracy: 0.5103 - val_loss: 1.0350\n",
      "Epoch 9/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 722ms/step - accuracy: 0.7780 - loss: 0.4125 - val_accuracy: 0.5339 - val_loss: 1.1425\n",
      "Epoch 10/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 869ms/step - accuracy: 0.8356 - loss: 0.2843 - val_accuracy: 0.6421 - val_loss: 1.2245\n",
      "Epoch 11/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 803ms/step - accuracy: 0.8784 - loss: 0.2243 - val_accuracy: 0.6119 - val_loss: 1.3448\n",
      "Epoch 12/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 768ms/step - accuracy: 0.9143 - loss: 0.1663 - val_accuracy: 0.6203 - val_loss: 1.3923\n",
      "Epoch 13/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 765ms/step - accuracy: 0.9237 - loss: 0.1389 - val_accuracy: 0.6137 - val_loss: 1.5002\n",
      "Epoch 14/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 750ms/step - accuracy: 0.9254 - loss: 0.1229 - val_accuracy: 0.6348 - val_loss: 1.5256\n",
      "Epoch 15/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 790ms/step - accuracy: 0.9354 - loss: 0.1134 - val_accuracy: 0.6131 - val_loss: 1.6638\n",
      "Epoch 16/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 770ms/step - accuracy: 0.9581 - loss: 0.0921 - val_accuracy: 0.6844 - val_loss: 1.6531\n",
      "Epoch 17/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 722ms/step - accuracy: 0.9517 - loss: 0.0912 - val_accuracy: 0.7056 - val_loss: 2.0710\n",
      "Epoch 18/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 743ms/step - accuracy: 0.9554 - loss: 0.0869 - val_accuracy: 0.6632 - val_loss: 1.8127\n",
      "Epoch 19/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 793ms/step - accuracy: 0.9562 - loss: 0.0746 - val_accuracy: 0.6651 - val_loss: 1.8420\n",
      "Epoch 20/20\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 789ms/step - accuracy: 0.9464 - loss: 0.0973 - val_accuracy: 0.6971 - val_loss: 1.9732\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.6988 - loss: 1.8882\n",
      "Test Accuracy: 68.96%\n"
     ]
    }
   ],
   "source": [
    "# === Compute Class Weights ===\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# === CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Early Stopping Callback ===\n",
    "\"\"\"\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=10,  # Wait for 10 epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore the weights from the best epoch\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# === Train Model with Class Weights and Early Stopping ===\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights_dict,\n",
    "    # callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# === Evaluate Model ===\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22f346c-3dfc-4d02-b6f6-0b8b14d23723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step\n",
      "Confusion Matrix:\n",
      "[[  15  228   20]\n",
      " [ 143 2782  170]\n",
      " [  40  682   53]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    COVID-19       0.08      0.06      0.07       263\n",
      "     Healthy       0.75      0.90      0.82      3095\n",
      " Symptomatic       0.22      0.07      0.10       775\n",
      "\n",
      "    accuracy                           0.69      4133\n",
      "   macro avg       0.35      0.34      0.33      4133\n",
      "weighted avg       0.61      0.69      0.64      4133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# === Predict on test set ===\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# === Classification Report (optional, includes precision, recall, f1) ===\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['COVID-19', 'Healthy', 'Symptomatic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6400e-3e34-4965-97b4-ca397e36e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 5.237959442332066, 1: 0.4450636728320276, 2: 1.7786744136001722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# === Compute Class Weights ===\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# === CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# === Evaluate Model ===\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f4973-6d3a-410c-ba21-5b488480c7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
